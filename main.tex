\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}

% graphicx
\usepackage{graphicx}
\graphicspath{{images/}}

% biblatex
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[nottoc]{tocbibind}

\title{
    {Deep Q-learning in Grid Worlds}
}
\author{Mihnea Ungureanu}

\begin{document}

\maketitle

\chapter*{Abstract}
Over the relatively recent years, \textbf{reinforcement learning (RL)} has gained immense popularity.
A multitude of interesting results in this field focuses on agents learning in simple, game-like, 2D grid-based environments.

In this paper, we focus on one particular technique in RL -- Q-learning -- and its applications in grid-based, game-like environments.

Classic Q-learning was an early breaktrhough in RL.
We document its evolution and build a survey of improvements over the original.
These techniques are used in practice and have good results in certain areas, such as playing Atari games.
We give examples of succesful applications and explain why Q-learning played a key role.
Finally, we look at advantages and disadvantages over other techniques in RL.

\tableofcontents

\chapter{Short Introduction to RL}
\input{chapters/rl}

\chapter{Q-learning and Deep Q-networks}
\input{chapters/dql}

\chapter{Application in Grid Worlds and 2D Games}
\input{chapters/application}

\chapter{Conclusion}
\input{chapters/conclusion}

\bibliography{references}

\end{document}