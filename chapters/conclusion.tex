The transfer of knowledge from deep learning to the classical methods in RL is revolutionizing the field.
Q-learning has evolved a long way since its inception in 1992.
Off-policy TD-control methods have shown a promising evolution, shifting from inefficient tabular representations to neural networks and other powerful ML models.

At the same time, despite being marked by the success of AlphaGo and its variants, RL agents are not yet prepared to handle real-world problems.

The study of autonomously learning RL agents in games has also been challenging.
As we have seen with DQN and its successors, its greatest weakness remains sample efficiency -- agents need to train for millions of steps before they demonstrate any significant amount of ability.
And even then -- as pay closer attention to generalization in deep RL -- we find that most methods fail to manifest the ability to generalize, outside of very simple strategies (such as Breakout or Pong).

With this in mind, the future looks bright, with a new recently released paper from DeepMind -- \emph{Agent57} (2020) \cite{agent57-paper}.
The latest research trends seem to be using mixed approaches: DQN and policy optimization algorithms, in an attempt to break the barriers set by their predecessors.
Agent57 has managed to break every record on the Atari benchmark and has obtained scores above the human baseline on each of the 57 games in the set.
