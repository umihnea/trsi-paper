\section{Q-learning and Q-networks}

Q-learning \cite{Watkins1992} appeared in 1992 and is considered one of the early breakthroughs of RL \cite{rlai}.
It is a simple algorithm allowing an agent to learn an optimal policy in an unknown environment.

Q-learning is part of \textbf{temporal difference (TD)} learning.
TD is a class of solution methods in RL that is part of the model-free subset \cite{rlai}.
An advantage of TD learning can learn from incomplete episodes \cite{long-peak-rl}.

\subsection{Q-learning and the Q function}
In a Q-learning problem, the focus of our optimization is explicitly on actions, depending on the state they are taken in.
What we defined in \ref{rl:value-func} is a state-value function, as it maps each state to a value representing its desirability.
The Q function in Q-learning, in contrast, is an \emph{action-value function}.

An action-value function \(Q(s, a)\) (for ``quality'') takes a \emph{state-action pair} as input (not just a state).
Its value is the expected return of taking action \(a\) in state \(s\), then continuing to act based on the current policy.
This value can actually be decomposed into two parts: immediate reward \(r\) given by state \(s\), and the value of the successor state \({s}'\) \cite{silver-lectures}.
This idea is captured in the Bellman Equations, which we will omit here for brevity, but can be found in \cite{rlai, silver-lectures}.

A Q-learning agent chooses actions greedily based on their Q-values.
An optimal Q-function, denoted by  \(Q^{\star}(s, a)\), corresponds to an optimal policy (optimal behaviour for the agent).
Through the iterative process of Q-learning, our \(Q\) eventually converges to \(Q^{\star}\).

Q-learning is useful in problems with a finite number of states and a finite set of possible actions.
In the most trivial examples, the Q function can be represented as a table, memorizing state and action.

\subsection{Q-networks}

Using a dictionary (data structure) to map state-action pairs to their values breaks down quickly in practice.
Instead, we need a more powerful model to store and update our Q-function.
A \textbf{neural network} can fill in the role of the function approximator.
This solves the data efficiency problem of Q-learning \cite{long-peak-rl}.
We call this a \textbf{Q-network}, or \textbf{deep Q-network}, denoting it is multi-layered.
``The Q-network can be a multi-layer dense neural network, a convolutional network, or a recurrent network, depending on the problem'' \cite{long-peak-rl}.

\section{Atari DQN}

Unfortunately, using a neural network to approximate the Q-function introduces additional complexity to the problem.
Whereas classical Q-learning is shown to be stable and manages to converge to the optimal values under easily-met conditions \cite{atari-dqn},
taking the Q-network approach may introduce \textbf{instability} and \textbf{divergence} \cite{long-peak-rl}.

\textbf{Atari DQN} \cite{atari-dqn} tackles those issues by introducing two key concepts: experience replay and a target network. In the upcoming section, we explain these crucial improvements.

\subsection{Experience Replay}

Experience replay refers to keeping a replay queue in memory and sampling episodes at random from this queue \cite{atari-dqn}.

Firstly, experience replay boosts the sample efficiency of Q-learning. Q-learning is an online learning algorithm: given a sample, it will consume it to adjust its values.
This immediate discarding can lead to forgetting, thus producing suboptimal behaviour.
Storing samples overcomes this problem.

Secondly, random sampling is important due to the nature of the data.
To explain why replay grants a significant advantage, let us consider a more popular use-case for neural networks: classification problems.
In supervised learning, a correct training set contains samples that are \emph{uniformly distribuited}.
Our problem differs significanlty in definition: our input data is sequential.
(Game states obtained from the Atari emulator are highly correlated.)
Using highly correlated data to train our Q-network can easily lead to overfitting \cite{jaromiru-dqn}.
Thus, it is better to sample episodes at random instead of training in an ordered sequence.

In DQN, the capacity of the replay memory is limited, as specified by a hyperparameter \(N\). When the memory is full, it discards the oldest sample.

\subsection{Target Network}

In Q-learning, the \(Q\) function is updated based on its own values.
In a Q-network, this would translate to updating the network based on its own parameters. This can introduce instability and divergence \cite{jaromiru-dqn}.

The idea of a target network is simple in principle:
every \(t\) time-steps, we take a snapshot of the main network's parameters and use them to create a \textbf{target network}.
The target network is \emph{frozen} \cite{long-peak-rl} until the next snapshot.
Between snapshots, \(Q\) is optimized based on values of the target.
This significanlty reduces oscillations, increasing stability of learning \cite{atari-dqn}.

\section{Rainbow DQN: The state-of-the-art}
% * show the graph in the rainbow paper
% * a series of improvements: list them all
% * take 2 subsections to explain 2 easier improvements?
% * mention Sonic the Hedgehog(tm) benchmark and cite it